# ğŸš€ ClasificaciÃ³n de Empleados: PredicciÃ³n de Attrition

## DescripciÃ³n del Proyecto ğŸ“Š

Este proyecto tiene como objetivo clasificar empleados con base en la probabilidad de *attrition* (rotaciÃ³n o deserciÃ³n) dentro de una empresa. Utilizando datos reales de recursos humanos, se desarrollan modelos de machine learning capaces de identificar patrones que conducen a la deserciÃ³n de empleados, permitiendo a la empresa tomar decisiones proactivas y mejorar la retenciÃ³n de talento. El repositorio completo se encuentra disponible en [GitHub](https://github.com/yanruwu/Proyecto8-Clasificacion).

### Proceso del Proyecto ğŸš€

1. **AnÃ¡lisis Exploratorio de Datos (EDA)**:
   - El primer paso fue realizar un anÃ¡lisis exploratorio de datos (EDA) para comprender mejor la naturaleza de los datos y tratar valores nulos e inconsistencias. Para ello, se utilizÃ³ `pandas` para el manejo de datos, y `matplotlib` y `seaborn` para generar visualizaciones que permitieran identificar patrones y correlaciones entre variables relevantes.

2. **Preprocesamiento de Datos**:
   - En la etapa de preprocesamiento, se llevaron a cabo varias actividades fundamentales para preparar los datos. Se empleÃ³ `support_prep.py` en la carpeta `src/` para realizar la codificaciÃ³n de variables categÃ³ricas (one-hot encoding con el apoyo del archivo `onehot.pkl` y target encoding, dependiendo de si la variable era nominal u ordinal, utilizando `target.pkl`), la normalizaciÃ³n de las caracterÃ­sticas (feature scaling) con el escalador almacenado en `scaler.pkl`, y la detecciÃ³n y tratamiento de outliers para garantizar que el modelo no se viera afectado por valores extremos. TambiÃ©n se emplearon tÃ©cnicas para eliminar duplicados y gestionar el desbalanceo en los datos.

3. **IteraciÃ³n de Modelos**:
   - Se llevaron a cabo diferentes iteraciones de modelos de machine learning. Se entrenaron varios modelos, incluyendo `Logistic Regression`, `Decision Tree`, `Random Forest`, `Gradient Boosting`, `XGBoost` y `Support Vector Classifier (SVC)`. Se realizaron tres fases de iteraciones:
     - Con los datos originales.
     - Eliminando duplicados.
     - Gestionando el desbalanceo mediante tÃ©cnicas de remuestreo (ver `model_resampled.pkl`).
   - Se utilizÃ³ `GridSearchCV` para optimizar los hiperparÃ¡metros de cada modelo y encontrar la configuraciÃ³n Ã³ptima para maximizar mÃ©tricas como `accuracy`, `precision`, `recall`, y `F1-score`.

4. **SelecciÃ³n del Mejor Modelo**:
   - Tras evaluar el rendimiento de todos los modelos, se seleccionÃ³ el mejor para su implementaciÃ³n. El modelo seleccionado fue `XGBoost`, el cual mostrÃ³ el mejor rendimiento en las mÃ©tricas consideradas. Este modelo se utilizÃ³ para crear una API mediante `Flask`, ubicada en la carpeta `api/`. Esta API permite acceder a las predicciones de forma sencilla y escalable.

5. **CreaciÃ³n de Webapp**:
   - Posteriormente, se desarrollÃ³ una aplicaciÃ³n web con `Streamlit` (`app/main.py`) que llama a la API de Flask y presenta de manera amigable los resultados a los usuarios. Esto permite a los departamentos de recursos humanos visualizar fÃ¡cilmente las predicciones y factores asociados al attrition de los empleados.
   - TambiÃ©n se intentÃ³ implementar una aplicaciÃ³n usando `Reflex` como frontend (cÃ³digo en `reflexapp/`), pero este intento no tuvo Ã©xito. Sin embargo, se considera retomarlo como parte de los prÃ³ximos pasos del proyecto.

## Estructura del Proyecto ğŸ—‚ï¸
```
â”œâ”€â”€ api/                        # CÃ³digo relacionado con la API del proyecto
â”‚   â””â”€â”€ main.py                 # CÃ³digo en Flask para la creaciÃ³n de la API
â”œâ”€â”€ app/                        # AplicaciÃ³n frontend para visualizar los resultados
â”‚   â””â”€â”€ main.py                 # CÃ³digo en Streamlit de la webapp que llama a la API de Flask
â”œâ”€â”€ datos/                      # Archivos CSV y datos en crudo
â”‚   â”œâ”€â”€ clean.pkl               # Datos preprocesados en formato pickle
â”‚   â”œâ”€â”€ diccionario-datos.xlsx  # Diccionario de datos con la descripciÃ³n de cada columna
â”‚   â”œâ”€â”€ employee_survey_data.csv # Datos de encuestas realizadas a empleados
â”‚   â”œâ”€â”€ general_data.csv        # Datos generales de los empleados
â”‚   â”œâ”€â”€ manager_survey_data.csv # Datos de encuestas realizadas a los gerentes
â”‚   â”œâ”€â”€ prepped.pkl             # Datos preparados para el modelado en formato pickle
â”‚   â”œâ”€â”€ prepped_nodup.pkl       # Datos preparados sin duplicados en formato pickle
â”œâ”€â”€ models/                     # Modelos entrenados y sus configuraciones
â”‚   â”œâ”€â”€ model_resampled.pkl     # Modelo entrenado utilizando tÃ©cnicas de remuestreo
â”‚   â”œâ”€â”€ onehot.pkl              # Codificador one-hot usado en el preprocesamiento
â”‚   â”œâ”€â”€ scaler.pkl              # Escalador utilizado para normalizar los datos
â”‚   â”œâ”€â”€ target.pkl              # Datos del target preprocesado para el modelado
â”œâ”€â”€ img/                        # ImÃ¡genes
â”œâ”€â”€ notebooks/                  # Notebooks Jupyter para EDA y desarrollo de modelos
â”‚   â”œâ”€â”€ 1-eda.ipynb             # AnÃ¡lisis exploratorio de los datos
â”‚   â”œâ”€â”€ 2-preprocess.ipynb      # Limpieza y preparaciÃ³n de los datos
â”‚   â”œâ”€â”€ 3.1-models.ipynb        # Entrenamiento inicial de modelos
â”‚   â”œâ”€â”€ 3.2-models_nodup.ipynb  # Modelos entrenados sin duplicados
â”‚   â”œâ”€â”€ 3.3-models_nodup_balanced.ipynb # Modelos entrenados sin duplicados con datos balanceados
â”‚   â””â”€â”€ api_tests.ipynb         # Pruebas para la API utilizando Jupyter
â”œâ”€â”€ reflexapp/                  # CÃ³digo para la implementaciÃ³n de Reflex como frontend
â”œâ”€â”€ src/                        # CÃ³digo fuente principal para el preprocesamiento y modelado
â”‚   â”œâ”€â”€ support_exchangerate.py # Funciones de soporte para el manejo de tasas de cambio
â”‚   â”œâ”€â”€ support_models.py       # Funciones de soporte para la creaciÃ³n y evaluaciÃ³n de modelos
â”‚   â”œâ”€â”€ support_plots.py        # Funciones para la generaciÃ³n de grÃ¡ficos y visualizaciones
â”‚   â””â”€â”€ support_prep.py         # Funciones para la preparaciÃ³n y limpieza de los datos
â”œâ”€â”€ environment.yml             # Archivo de configuraciÃ³n para gestionar dependencias del entorno
â””â”€â”€ README.md                   # DocumentaciÃ³n del proyecto
```

## InstalaciÃ³n y Requisitos âš™ï¸

Para configurar el entorno de desarrollo y asegurarse de que todas las dependencias necesarias estÃ©n instaladas, se deben seguir estos pasos:

### Requisitos

- Python 3.8 o superior ğŸ
- [Anaconda](https://www.anaconda.com/products/distribution) o [Miniconda](https://docs.conda.io/en/latest/miniconda.html) (opcional, pero recomendado)

### Paquetes Necesarios

El proyecto utiliza los siguientes paquetes:

- [`pandas`](https://pandas.pydata.org/pandas-docs/stable/): Para la manipulaciÃ³n y anÃ¡lisis de datos.
- [`numpy`](https://numpy.org/doc/stable/): Para operaciones numÃ©ricas y manejo de arrays.
- [`scikit-learn`](https://scikit-learn.org/stable/): Para la creaciÃ³n y evaluaciÃ³n de modelos de machine learning.
- [`category_encoders`](https://pypi.org/project/category-encoders/): Para el target encoding.
- [`matplotlib`](https://matplotlib.org/stable/users/index.html): Para la visualizaciÃ³n de datos.
- [`seaborn`](https://seaborn.pydata.org/): Para visualizaciÃ³n estadÃ­stica de datos.
- [`imblearn`](https://imbalanced-learn.org/stable/): Para la gestiÃ³n de desbalance.
- [`shap`](https://shap.readthedocs.io/en/latest/): Para plots de resumen tipo SHAP.
- [`streamlit`](https://docs.streamlit.io/): Para creaciÃ³n de webapp desde Python.
- [`flax`](https://flask.palletsprojects.com/en/stable/): Para la creaciÃ³n de APIs desde Python.

### InstalaciÃ³n

1. **Clonar el repositorio:**

   ```bash
   git clone https://github.com/yanruwu/Proyecto8-Clasificacion
   cd Proyecto8-Clasificacion
   ```

2. **Crear un entorno virtual:**

   Para crear el entorno de Conda, usar el siguiente comando:

   ```bash
   conda env create -f environment.yml
   ```

   O si se prefiere usar venv:

   ```bash
   python -m venv venv
   source venv/bin/activate  # En macOS/Linux
   venv\Scripts\activate     # En Windows
   ```

### Ejecutar la API y la Webapp

- **API (Flask)**:

  - Navegar al directorio `api` y ejecutar `main.py` con el siguiente comando:
    ```bash
    python main.py
    ```

- **Webapp (Streamlit)**:

  - Navegar al directorio `app` y ejecutar `main.py` con el siguiente comando:
    ```bash
    streamlit run main.py
    ```


### Conclusiones

Este proyecto ha permitido identificar y entender los principales factores que influyen en la retenciÃ³n de empleados, utilizando un modelo Ã³ptimo basado en **XGBoost**. DespuÃ©s de eliminar duplicados y balancear las clases con **SMOTETomek**, el modelo ha mostrado una capacidad notable para predecir la probabilidad de **attrition**, proporcionando informaciÃ³n Ãºtil sobre los factores mÃ¡s determinantes para la retenciÃ³n.

- Las caracterÃ­sticas mÃ¡s importantes para la predicciÃ³n de la attrition son **Estado Civil**, **Frecuencia de Viajes**, **AÃ±os Totales de Experiencia** y **SatisfacciÃ³n con el Ambiente Laboral**. Estas variables indican que la estabilidad personal, la carga de viajes, la experiencia acumulada y el nivel de satisfacciÃ³n con el entorno laboral juegan un papel crucial en la decisiÃ³n de los empleados de permanecer en la empresa.

- El **SHAP Summary Plot** nos ha permitido entender cÃ³mo las diferentes caracterÃ­sticas afectan la predicciÃ³n del modelo. Factores como una mayor **satisfacciÃ³n laboral**, **menos empresas previas**, **edad avanzada** y **promociones recientes** tienden a tener un impacto positivo en la retenciÃ³n, mientras que una **falta de promociÃ³n** y **baja satisfacciÃ³n con el ambiente** tienen un efecto negativo.

![Texto Alternativo](img\shap.png)

- Aunque el modelo XGBoost presenta un **recall** alto y proporciona buenas predicciones, tambiÃ©n se observÃ³ cierta tendencia al **overfitting** cuando se entrenÃ³ con datos balanceados, lo que implica que podrÃ­a ser necesario ajustar mejor los hiperparÃ¡metros o explorar otros enfoques para mejorar la capacidad de generalizaciÃ³n.

| Dataset | Precision | Accuracy | Recall | F1 Score | Kappa |
|---------|-----------|----------|--------|----------|-------|
| train   | 1.000000  | 0.999441 | 0.998889 | 0.999444 | 0.998882 |
| test    | 0.921127  | 0.897001 | 0.865079 | 0.892224 | 0.793783 |



- AdemÃ¡s de la iteraciÃ³n con datos balanceados y sin duplicados, se realizaron otras iteraciones con el conjunto completo de datos y con datos solo sin duplicados. Estas iteraciones permitieron observar las diferencias en el rendimiento del modelo, evidenciando que la eliminaciÃ³n de duplicados y el balanceo afectan significativamente la capacidad de generalizaciÃ³n y la estabilidad de las predicciones.

Este anÃ¡lisis podrÃ­a ser una base sÃ³lida para implementar polÃ­ticas que mejoren la satisfacciÃ³n laboral y reduzcan la rotaciÃ³n de personal.



## PrÃ³ximos Pasos ğŸ”

1. **Implementar un Dashboard**:
   - Desarrollar un dashboard interactivo para visualizar la probabilidad de attrition y los factores relevantes en tiempo real.

2. **Explorar el Uso de Reflex**:
   - Se intentarÃ¡ nuevamente implementar una aplicaciÃ³n utilizando Reflex como frontend. El objetivo es mejorar la interfaz y la experiencia del usuario, considerando las limitaciones encontradas previamente.
3. **ImplementaciÃ³n de exchange**:
    - Implementar una API que permita seleccionar el tipo de moneda que se usa en la predicciÃ³n de la permanencia (soporte)

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Para mejorar el proyecto, se puede abrir un pull request o una issue.

